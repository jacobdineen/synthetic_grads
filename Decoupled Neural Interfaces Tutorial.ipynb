{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://github.com/nitarshan/decoupled-neural-interfaces\n",
    "# Decoupled Neural Interfaces\n",
    "\n",
    "An implementation of DNIs in TensorFlow. As in the referenced paper, the feasability of this technique is demonstrated through the use of stochastic layer-wise updates when training a fully connected network on the MNIST classification problem.\n",
    "\n",
    "Reference: [Decoupled Neural Interfaces using Synthetic Gradients](https://arxiv.org/abs/1608.05343)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T23:35:48.088014Z",
     "start_time": "2021-05-06T23:35:45.815305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\hicss\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tensorflow.compat.v1 as tf\n",
    "from keras.utils.np_utils import to_categorical   \n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "from tqdm import tqdm # Used to display training progress bar\n",
    "\n",
    "sg_sess = tf.Session(\"grpc://localhost:2222\")\n",
    "backprop_sess = tf.Session(\"grpc://localhost:2223\")\n",
    "\n",
    "cluster = tf.train.ClusterSpec({\"local\": [\"localhost:2222\", \"localhost:2223\"]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture\n",
    "Architecture and training largely follow the details provided in section **A.1 Feed-Forward Implementation Details** of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T23:35:52.985196Z",
     "start_time": "2021-05-06T23:35:52.752385Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data scaled to [0,1] interval, and labels in one-hot format\n",
    "# 55k train, 5k validation, 10k test\n",
    "(xs, ys), (xs_, ys_) = tf.keras.datasets.mnist.load_data(\n",
    "    path='mnist.npz'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "xs = xs.reshape(60000,784)[:500]\n",
    "xs_ = xs_.reshape(10000,784)[:500]\n",
    "ys = to_categorical(ys, 10)[:500]\n",
    "ys_= to_categorical(ys_, 10)[:500]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T23:35:54.393649Z",
     "start_time": "2021-05-06T23:35:54.385639Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "iterations = 150000\n",
    "batch_size = 1000 # modified to evenly divide dataset size\n",
    "\n",
    "init_lr = 3e-5\n",
    "lr_div = 10\n",
    "lr_div_steps = set([300000, 400000])\n",
    "\n",
    "update_prob = 0.2 # Probability of updating a decoupled layer\n",
    "\n",
    "validation_checkpoint = 10 # How often (iterations) to validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T23:35:55.493991Z",
     "start_time": "2021-05-06T23:35:55.488990Z"
    }
   },
   "outputs": [],
   "source": [
    "# Functions for constructing layers\n",
    "def dense_layer(inputs, units, name, output=False):\n",
    "    with tf.variable_scope(name):\n",
    "        x = tf.layers.dense(inputs, units, name=\"fc\")\n",
    "        if not output:\n",
    "            x = tf.layers.batch_normalization(x, name=\"bn\")\n",
    "            x = tf.nn.relu(x, name=\"relu\")\n",
    "    return x\n",
    "\n",
    "def sg_module(inputs, units, name, label):\n",
    "    with tf.variable_scope(name):\n",
    "        inputs_c = tf.concat([inputs, label], 1)\n",
    "        x = tf.layers.dense(inputs_c, units, name=\"fc\", kernel_initializer=tf.zeros_initializer())\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T23:35:57.698073Z",
     "start_time": "2021-05-06T23:35:57.548069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-601abdffde65>:4: dense (from tensorflow.python.keras.legacy_tf_layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\hicss\\lib\\site-packages\\tensorflow\\python\\keras\\legacy_tf_layers\\core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-601abdffde65>:6: batch_normalization (from tensorflow.python.keras.legacy_tf_layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n"
     ]
    }
   ],
   "source": [
    "# Ops for network architecture\n",
    "with tf.variable_scope(\"architecture\"):\n",
    "    # Inputs\n",
    "    with tf.variable_scope(\"input\"):\n",
    "        X = tf.placeholder(tf.float32, shape=(None, 784), name=\"data\") # Input\n",
    "        Y = tf.placeholder(tf.float32, shape=(None, 10), name=\"labels\") # Target\n",
    "    \n",
    "    # Inference layers\n",
    "    h1 = dense_layer(X, 256, \"layer1\")\n",
    "    h2 = dense_layer(h1, 256, name=\"layer2\")\n",
    "    h3 = dense_layer(h2, 256, name=\"layer3\")\n",
    "    logits = dense_layer(h3, 10, name=\"layer4\", output=True)\n",
    "    \n",
    "    # Synthetic Gradient layers\n",
    "    d1_hat = sg_module(h1, 256, \"sg2\", Y)\n",
    "    d2_hat = sg_module(h2, 256, \"sg3\", Y)\n",
    "    d3_hat = sg_module(h3, 256, \"sg4\", Y)\n",
    "\n",
    "# Collections of trainable variables in each block\n",
    "layer_vars = [tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/layer1/\"),\n",
    "              tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/layer2/\"),\n",
    "              tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/layer3/\"),\n",
    "              tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/layer4/\")]\n",
    "sg_vars = [None,\n",
    "           tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/sg2/\"),\n",
    "           tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/sg3/\"),\n",
    "           tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/sg4/\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T23:35:59.135983Z",
     "start_time": "2021-05-06T23:35:58.450412Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function for optimizing a layer and its synthetic gradient module\n",
    "def train_layer_n(n, h_m, h_n, d_hat_m, class_loss, d_n=None):\n",
    "    with tf.variable_scope(\"layer\"+str(n)):\n",
    "        layer_grads = tf.gradients(h_n, [h_m]+layer_vars[n-1], d_n)\n",
    "        layer_gv = list(zip(layer_grads[1:],layer_vars[n-1]))\n",
    "        layer_opt = tf.train.AdamOptimizer(learning_rate=learning_rate).apply_gradients(layer_gv)\n",
    "    with tf.variable_scope(\"sg\"+str(n)):\n",
    "        d_m = layer_grads[0]\n",
    "        sg_loss = tf.divide(tf.losses.mean_squared_error(d_hat_m, d_m), class_loss)\n",
    "        sg_opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(sg_loss, var_list=sg_vars[n-1])\n",
    "    return layer_opt, sg_opt\n",
    "\n",
    "# Ops for training\n",
    "with tf.variable_scope(\"train\"):\n",
    "    with tf.variable_scope(\"learning_rate\"):\n",
    "        learning_rate = tf.Variable(init_lr, dtype=tf.float32, name=\"lr\")\n",
    "        reduce_lr = tf.assign(learning_rate, learning_rate/lr_div, name=\"lr_decrease\")\n",
    "\n",
    "    pred_loss = tf.losses.softmax_cross_entropy(onehot_labels=Y, logits=logits, scope=\"prediction_loss\")\n",
    "    \n",
    "    # Optimizers when using synthetic gradients\n",
    "    with tf.variable_scope(\"synthetic\"):\n",
    "        layer4_opt, sg4_opt = train_layer_n(4, h3, pred_loss, d3_hat, pred_loss)\n",
    "        layer3_opt, sg3_opt = train_layer_n(3, h2, h3, d2_hat, pred_loss, d3_hat)\n",
    "        layer2_opt, sg2_opt = train_layer_n(2, h1, h2, d1_hat, pred_loss, d2_hat)\n",
    "        with tf.variable_scope(\"layer1\"):\n",
    "            layer1_opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(h1, var_list=layer_vars[0], grad_loss=d1_hat)\n",
    "    \n",
    "    # Optimizer when using backprop\n",
    "    with tf.variable_scope(\"backprop\"):\n",
    "        backprop_opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(pred_loss)\n",
    "        \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T23:35:59.938237Z",
     "start_time": "2021-05-06T23:35:59.922315Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ops for validation and testing (computing classification accuracy)\n",
    "with tf.variable_scope(\"test\"):\n",
    "    preds = tf.nn.softmax(logits, name=\"predictions\")\n",
    "    correct_preds = tf.equal(tf.argmax(preds,1), tf.argmax(Y,1), name=\"correct_predictions\")\n",
    "    accuracy = tf.reduce_sum(tf.cast(correct_preds,tf.float32), name=\"correct_prediction_count\") / 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T23:36:00.748620Z",
     "start_time": "2021-05-06T23:36:00.733589Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ops for tensorboard summary data\n",
    "with tf.variable_scope(\"summary\"):\n",
    "    cost_summary_opt = tf.summary.scalar(\"loss\", pred_loss)\n",
    "    accuracy_summary_opt = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "With training using backprop, each layer cannot be updated until the gradient from the loss is calculated for the following layer. This update-locking is avoided when using synthetic gradients, as the next layer's gradients are approximated by doing a forward pass through that layer's synthetic gradient module. However, synthetic gradient modules themselves are update-locked to the layer they are grouped with, as updating the gradient estimation will require the true gradient which we only calculate when updating the layer itself. Though not used as such here, the benefit of using synthetic gradients is that it allows training of layers to be parallelized across many GPUs, which would be a significant speedup when training large models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T00:02:57.565267Z",
     "start_time": "2021-05-06T23:37:30.787254Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 150000/150000 [25:26<00:00, 98.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train using backprop as benchmark\n",
    "with backprop_sess.as_default():\n",
    "    backprop_train_writer = tf.summary.FileWriter(\"logging1/backprop/train\")\n",
    "    backprop_validation_writer = tf.summary.FileWriter(\"logging1/backprop/validation\")\n",
    "\n",
    "    backprop_sess.run(init)\n",
    "    for i in tqdm(range(1,iterations+1)):\n",
    "        if i in lr_div_steps: # Decrease learning rate\n",
    "            backprop_sess.run(reduce_lr)\n",
    "        \n",
    "        data, target = xs, ys\n",
    "        _, summary = backprop_sess.run([backprop_opt, summary_op], feed_dict={X:data,Y:target})\n",
    "        backprop_train_writer.add_summary(summary, i)\n",
    "        \n",
    "        if i % validation_checkpoint == 0:\n",
    "            Xb, Yb = xs_, ys_\n",
    "            summary = backprop_sess.run([summary_op], feed_dict={X:Xb,Y:Yb})[0]\n",
    "            backprop_validation_writer.add_summary(summary, i)\n",
    "\n",
    "    # Cleanup summary writers\n",
    "    backprop_train_writer.close()\n",
    "    backprop_validation_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T00:18:34.675518Z",
     "start_time": "2021-05-07T00:02:57.582270Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 150000/150000 [15:35<00:00, 160.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train using synthetic gradients\n",
    "with sg_sess.as_default():\n",
    "    sg_train_writer = tf.summary.FileWriter(\"logging1/sg/train\", sg_sess.graph)\n",
    "    sg_validation_writer = tf.summary.FileWriter(\"logging1/sg/validation\")\n",
    "    \n",
    "    sg_sess.run(init)\n",
    "    for i in tqdm(range(1,iterations+1)):\n",
    "        if i in lr_div_steps: # Decrease learning rate\n",
    "            sg_sess.run(reduce_lr)\n",
    "        \n",
    "        data, target = xs, ys\n",
    "        with tf.device(\"/job:local/task:0\"):\n",
    "            # Each layer can now be independently updated (could be parallelized)\n",
    "            if random.random() <= update_prob: # Stochastic updates are possible\n",
    "                sg_sess.run([layer1_opt], feed_dict={X:data,Y:target})\n",
    "            if random.random() <= update_prob:\n",
    "                sg_sess.run([layer2_opt, sg2_opt], feed_dict={X:data,Y:target})\n",
    "        with tf.device(\"/job:local/task:1\"):\n",
    "            if random.random() <= update_prob:\n",
    "                sg_sess.run([layer3_opt, sg3_opt], feed_dict={X:data,Y:target})\n",
    "            if random.random() <= update_prob:\n",
    "                _, _, summary = sg_sess.run([layer4_opt, sg4_opt, summary_op], feed_dict={X:data,Y:target})\n",
    "                sg_train_writer.add_summary(summary, i)\n",
    "        \n",
    "        if i % validation_checkpoint == 0:\n",
    "            Xb, Yb = xs_, ys_\n",
    "            summary = sg_sess.run([summary_op], feed_dict={X:Xb,Y:Yb})[0]\n",
    "            sg_validation_writer.add_summary(summary, i)\n",
    "    \n",
    "    # Cleanup summary writers\n",
    "    sg_train_writer.close()\n",
    "    sg_validation_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphed on TensorBoard, we see that the stochastically updated network manages to reach the validation accuracy of the network trained using backprop:  \n",
    "\n",
    "Orange - backprop/train  \n",
    "Light Blue - backprop/validate  \n",
    "Purple - sg/train  \n",
    "Dark Blue - sg/validate\n",
    "\n",
    "![Accuracy](images/dni_accuracy.png) ![Loss](images/dni_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T22:32:32.228704Z",
     "start_time": "2021-05-06T22:32:32.208673Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "sg_sess.close()\n",
    "backprop_sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T23:11:30.468225Z",
     "start_time": "2021-05-06T23:10:30.109932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ERROR: Timed out waiting for TensorBoard to start. It may still be running as pid 8588."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logging/sg/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
