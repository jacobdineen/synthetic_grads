{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://github.com/nitarshan/decoupled-neural-interfaces\n",
    "# Decoupled Neural Interfaces\n",
    "\n",
    "An implementation of DNIs in TensorFlow. As in the referenced paper, the feasability of this technique is demonstrated through the use of stochastic layer-wise updates when training a fully connected network on the MNIST classification problem.\n",
    "\n",
    "Reference: [Decoupled Neural Interfaces using Synthetic Gradients](https://arxiv.org/abs/1608.05343)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T16:48:43.681732Z",
     "start_time": "2021-05-20T16:48:43.668761Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow.compat.v1 as tf\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "from tqdm import tqdm  # Used to display training progress bar\n",
    "\n",
    "sg_sess_1 = tf.Session(\"grpc://localhost:2222\")\n",
    "sg_sess_2 = tf.Session(\"grpc://localhost:2223\")\n",
    "backprop_sess = tf.Session(\"grpc://localhost:2223\")\n",
    "\n",
    "cluster = tf.train.ClusterSpec({\"local\": [\"localhost:2222\", \"localhost:2223\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T16:42:38.559798Z",
     "start_time": "2021-05-20T16:42:38.334688Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data scaled to [0,1] interval, and labels in one-hot format\n",
    "# 55k train, 5k validation, 10k test\n",
    "(xs, ys), (xs_, ys_) = tf.keras.datasets.mnist.load_data(path='mnist.npz')\n",
    "\n",
    "xs = xs.reshape(60000, 784)[:500]\n",
    "xs_ = xs_.reshape(10000, 784)[:500]\n",
    "ys = to_categorical(ys, 10)[:500]\n",
    "ys_ = to_categorical(ys_, 10)[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T16:42:39.088855Z",
     "start_time": "2021-05-20T16:42:39.072826Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "iterations = 150000\n",
    "batch_size = 500  # modified to evenly divide dataset size\n",
    "\n",
    "init_lr = 3e-5\n",
    "lr_div = 10\n",
    "lr_div_steps = set([300000, 400000])\n",
    "\n",
    "update_prob = 0.2\n",
    "validation_checkpoint = 10  # How often (iterations) to validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T16:42:39.970151Z",
     "start_time": "2021-05-20T16:42:39.957124Z"
    }
   },
   "outputs": [],
   "source": [
    "# Functions for constructing layers\n",
    "def dense_layer(inputs, units, name, output=False):\n",
    "    with tf.compat.v1.variable_scope(name):\n",
    "        x = tf.layers.dense(inputs, units, name=\"fc\")\n",
    "        if not output:\n",
    "            x = tf.layers.batch_normalization(x, name=\"bn\")\n",
    "            x = tf.nn.relu(x, name=\"relu\")\n",
    "    return x\n",
    "\n",
    "\n",
    "def sg_module(inputs, units, name, label):\n",
    "    with tf.compat.v1.variable_scope(name):\n",
    "        inputs_c = tf.concat([inputs, label], 1)\n",
    "        x = tf.layers.dense(inputs_c,\n",
    "                            units,\n",
    "                            name=\"fc\",\n",
    "                            kernel_initializer=tf.zeros_initializer())\n",
    "    return x\n",
    "\n",
    "\n",
    "def train_layer_n(n, h_m, h_n, d_hat_m, class_loss, d_n=None):\n",
    "    with tf.variable_scope(\"layer\" + str(n)):\n",
    "        layer_grads = tf.gradients(h_n, [h_m] + layer_vars[n - 1], d_n)\n",
    "        layer_gv = list(zip(layer_grads[1:], layer_vars[n - 1]))\n",
    "        layer_opt = tf.train.AdamOptimizer(\n",
    "            learning_rate=learning_rate).apply_gradients(layer_gv)\n",
    "    with tf.variable_scope(\"sg\" + str(n)):\n",
    "        d_m = layer_grads[0]\n",
    "        sg_loss = tf.divide(tf.losses.mean_squared_error(d_hat_m, d_m),\n",
    "                            class_loss)\n",
    "        sg_opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(\n",
    "            sg_loss, var_list=sg_vars[n - 1])\n",
    "    return layer_opt, sg_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T16:42:41.447047Z",
     "start_time": "2021-05-20T16:42:41.314417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-ccd745e45ea0>:4: dense (from tensorflow.python.keras.legacy_tf_layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\hicss\\lib\\site-packages\\tensorflow\\python\\keras\\legacy_tf_layers\\core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-ccd745e45ea0>:6: batch_normalization (from tensorflow.python.keras.legacy_tf_layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"architecture\"):\n",
    "    # Inputs\n",
    "    with tf.variable_scope(\"input\"):\n",
    "        X = tf.placeholder(tf.float32, shape=(None, 784), name=\"data\")  # Input\n",
    "        Y = tf.placeholder(tf.float32, shape=(None, 10),\n",
    "                           name=\"labels\")  # Target\n",
    "\n",
    "    # Inference layers\n",
    "    h1 = dense_layer(X, 256, \"layer1\")\n",
    "    h2 = dense_layer(h1, 256, name=\"layer2\")\n",
    "    h3 = dense_layer(h2, 256, name=\"layer3\")\n",
    "    logits = dense_layer(h3, 10, name=\"layer4\", output=True)\n",
    "\n",
    "    # Synthetic Gradient layers\n",
    "    d1_hat = sg_module(h1, 256, \"sg2\", Y)\n",
    "    d2_hat = sg_module(h2, 256, \"sg3\", Y)\n",
    "    d3_hat = sg_module(h3, 256, \"sg4\", Y)\n",
    "\n",
    "# Collections of trainable variables in each block\n",
    "layer_vars = [\n",
    "    tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                      scope=\"architecture/layer1/\"),\n",
    "    tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                      scope=\"architecture/layer2/\"),\n",
    "    tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                      scope=\"architecture/layer3/\"),\n",
    "    tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                      scope=\"architecture/layer4/\")\n",
    "]\n",
    "sg_vars = [\n",
    "    None,\n",
    "    tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                      scope=\"architecture/sg2/\"),\n",
    "    tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                      scope=\"architecture/sg3/\"),\n",
    "    tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                      scope=\"architecture/sg4/\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T16:42:42.888518Z",
     "start_time": "2021-05-20T16:42:42.301851Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ops for training\n",
    "with tf.variable_scope(\"train\"):\n",
    "    with tf.variable_scope(\"learning_rate\"):\n",
    "        learning_rate = tf.Variable(init_lr, dtype=tf.float32, name=\"lr\")\n",
    "        reduce_lr = tf.assign(learning_rate,\n",
    "                              learning_rate / lr_div,\n",
    "                              name=\"lr_decrease\")\n",
    "\n",
    "    pred_loss = tf.losses.softmax_cross_entropy(onehot_labels=Y,\n",
    "                                                logits=logits,\n",
    "                                                scope=\"prediction_loss\")\n",
    "\n",
    "    # Optimizers when using synthetic gradients\n",
    "    with tf.variable_scope(\"synthetic\"):\n",
    "        layer4_opt, sg4_opt = train_layer_n(4, h3, pred_loss, d3_hat,\n",
    "                                            pred_loss)\n",
    "        layer3_opt, sg3_opt = train_layer_n(3, h2, h3, d2_hat, pred_loss,\n",
    "                                            d3_hat)\n",
    "        layer2_opt, sg2_opt = train_layer_n(2, h1, h2, d1_hat, pred_loss,\n",
    "                                            d2_hat)\n",
    "        with tf.variable_scope(\"layer1\"):\n",
    "            layer1_opt = tf.train.AdamOptimizer(\n",
    "                learning_rate=learning_rate).minimize(h1,\n",
    "                                                      var_list=layer_vars[0],\n",
    "                                                      grad_loss=d1_hat)\n",
    "\n",
    "    # Optimizer when using backprop\n",
    "    with tf.variable_scope(\"backprop\"):\n",
    "        backprop_opt = tf.train.AdamOptimizer(\n",
    "            learning_rate=learning_rate).minimize(pred_loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T16:42:43.537912Z",
     "start_time": "2021-05-20T16:42:43.516921Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ops for validation and testing (computing classification accuracy)\n",
    "with tf.variable_scope(\"test\"):\n",
    "    preds = tf.nn.softmax(logits, name=\"predictions\")\n",
    "    correct_preds = tf.equal(tf.argmax(preds, 1),\n",
    "                             tf.argmax(Y, 1),\n",
    "                             name=\"correct_predictions\")\n",
    "    accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32),\n",
    "                             name=\"correct_prediction_count\") / 500\n",
    "\n",
    "# Ops for tensorboard summary data\n",
    "with tf.variable_scope(\"summary\"):\n",
    "    cost_summary_opt = tf.summary.scalar(\"loss\", pred_loss)\n",
    "    accuracy_summary_opt = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T16:48:32.375390Z",
     "start_time": "2021-05-20T16:48:26.508434Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▋                                                                                                                                                                           | 572/150000 [00:05<25:24, 98.05it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-2f76366dd617>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackprop_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbackprop_opt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummary_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mbackprop_train_writer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\hicss\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    955\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 957\u001b[1;33m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[0;32m    958\u001b[0m                          run_metadata_ptr)\n\u001b[0;32m    959\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\hicss\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[0;32m   1181\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[0;32m   1182\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\hicss\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1358\u001b[1;33m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[0;32m   1359\u001b[0m                            run_metadata)\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\hicss\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\hicss\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1349\u001b[1;33m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[0;32m   1350\u001b[0m                                       target_list, run_metadata)\n\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\hicss\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1439\u001b[0m   def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,\n\u001b[0;32m   1440\u001b[0m                           run_metadata):\n\u001b[1;32m-> 1441\u001b[1;33m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1443\u001b[0m                                             run_metadata)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train using backprop as benchmark\n",
    "with backprop_sess.as_default():\n",
    "    backprop_train_writer = tf.summary.FileWriter(\"logging1/backprop/train\")\n",
    "    backprop_validation_writer = tf.summary.FileWriter(\n",
    "        \"logging1/backprop/validation\")\n",
    "\n",
    "    backprop_sess.run(init)\n",
    "    for i in tqdm(range(1, iterations + 1)):\n",
    "        if i in lr_div_steps:  # Decrease learning rate\n",
    "            backprop_sess.run(reduce_lr)\n",
    "\n",
    "        data, target = xs, ys\n",
    "        _, summary = backprop_sess.run([backprop_opt, summary_op],\n",
    "                                       feed_dict={\n",
    "                                           X: data,\n",
    "                                           Y: target\n",
    "                                       })\n",
    "        backprop_train_writer.add_summary(summary, i)\n",
    "\n",
    "        if i % validation_checkpoint == 0:\n",
    "            Xb, Yb = xs_, ys_\n",
    "            summary = backprop_sess.run([summary_op], feed_dict={\n",
    "                X: Xb,\n",
    "                Y: Yb\n",
    "            })[0]\n",
    "            backprop_validation_writer.add_summary(summary, i)\n",
    "\n",
    "    # Cleanup summary writers\n",
    "    backprop_train_writer.close()\n",
    "    backprop_validation_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T16:50:53.596024Z",
     "start_time": "2021-05-20T16:50:30.323535Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██▋                                                                                                                                                                       | 2392/150000 [00:22<22:44, 108.20it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-7fe8230d0eb9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0msg_sess_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m             \u001b[0msg_sess_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/job:local/task:1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mupdate_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\hicss\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    955\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 957\u001b[1;33m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[0;32m    958\u001b[0m                          run_metadata_ptr)\n\u001b[0;32m    959\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\hicss\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[0;32m   1181\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[0;32m   1182\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\hicss\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1358\u001b[1;33m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[0;32m   1359\u001b[0m                            run_metadata)\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\hicss\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\hicss\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1349\u001b[1;33m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[0;32m   1350\u001b[0m                                       target_list, run_metadata)\n\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\hicss\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1439\u001b[0m   def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,\n\u001b[0;32m   1440\u001b[0m                           run_metadata):\n\u001b[1;32m-> 1441\u001b[1;33m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1443\u001b[0m                                             run_metadata)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train using synthetic gradients\n",
    "with backprop_sess.as_default():\n",
    "    sg_train_writer = tf.summary.FileWriter(\"logging1/sg/train\",\n",
    "                                            backprop_sess.graph)\n",
    "    sg_validation_writer = tf.summary.FileWriter(\"logging1/sg/validation\")\n",
    "\n",
    "    sg_sess.run(init)\n",
    "    for i in tqdm(range(1, iterations + 1)):\n",
    "        if i in lr_div_steps:  # Decrease learning rate\n",
    "            sg_sess.run(reduce_lr)\n",
    "\n",
    "        data, target = xs, ys\n",
    "        with sg_sess_1.as_default():\n",
    "            sg_sess_1.run(init)\n",
    "            with tf.device(\"/job:local/task:0\"):\n",
    "                # Each layer can now be independently updated (could be parallelized)\n",
    "                if random.random(\n",
    "                ) <= update_prob:  # Stochastic updates are possible\n",
    "                    sg_sess_1.run([layer1_opt], feed_dict={X: data, Y: target})\n",
    "                if random.random() <= update_prob:\n",
    "                    sg_sess_1.run([layer2_opt, sg2_opt],\n",
    "                                feed_dict={\n",
    "                                    X: data,\n",
    "                                    Y: target\n",
    "                                })\n",
    "\n",
    "        with sg_sess_2.as_default():\n",
    "            sg_sess_2.run(init)\n",
    "            with tf.device(\"/job:local/task:1\"):\n",
    "                if random.random() <= update_prob:\n",
    "                    sg_sess_2.run([layer3_opt, sg3_opt],\n",
    "                                feed_dict={\n",
    "                                    X: data,\n",
    "                                    Y: target\n",
    "                                })\n",
    "                if random.random() <= update_prob:\n",
    "                    _, _, summary = sg_sess_2.run(\n",
    "                        [layer4_opt, sg4_opt, summary_op],\n",
    "                        feed_dict={\n",
    "                            X: data,\n",
    "                            Y: target\n",
    "                        })\n",
    "                    sg_train_writer.add_summary(summary, i)\n",
    "\n",
    "        if i % validation_checkpoint == 0:\n",
    "            Xb, Yb = xs_, ys_\n",
    "            summary = sg_sess.run([summary_op], feed_dict={X: Xb, Y: Yb})[0]\n",
    "            sg_validation_writer.add_summary(summary, i)\n",
    "\n",
    "    # Cleanup summary writers\n",
    "    sg_train_writer.close()\n",
    "    sg_validation_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T22:32:32.228704Z",
     "start_time": "2021-05-06T22:32:32.208673Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "sg_sess.close()\n",
    "backprop_sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T23:11:30.468225Z",
     "start_time": "2021-05-06T23:10:30.109932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ERROR: Timed out waiting for TensorBoard to start. It may still be running as pid 8588."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logging/sg/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
