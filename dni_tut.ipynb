{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://github.com/nitarshan/decoupled-neural-interfaces\n",
    "# Decoupled Neural Interfaces\n",
    "\n",
    "An implementation of DNIs in TensorFlow. As in the referenced paper, the feasability of this technique is demonstrated through the use of stochastic layer-wise updates when training a fully connected network on the MNIST classification problem.\n",
    "\n",
    "Reference: [Decoupled Neural Interfaces using Synthetic Gradients](https://arxiv.org/abs/1608.05343)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T17:48:29.778750Z",
     "start_time": "2021-05-20T17:48:27.707057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\hicss\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tensorflow.compat.v1 as tf\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "from tqdm import tqdm  # Used to display training progress bar\n",
    "\n",
    "sg_sess_1 = tf.Session(\"grpc://localhost:2222\")\n",
    "sg_sess_2 = tf.Session(\"grpc://localhost:2223\")\n",
    "backprop_sess = tf.Session(\"grpc://localhost:2223\")\n",
    "\n",
    "cluster = tf.train.ClusterSpec({\"local\": [\"localhost:2222\", \"localhost:2223\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T17:48:30.197322Z",
     "start_time": "2021-05-20T17:48:29.963477Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data scaled to [0,1] interval, and labels in one-hot format\n",
    "# 55k train, 5k validation, 10k test\n",
    "(xs, ys), (xs_, ys_) = tf.keras.datasets.mnist.load_data(path='mnist.npz')\n",
    "\n",
    "xs = xs.reshape(60000, 784)[:500]\n",
    "xs_ = xs_.reshape(10000, 784)[:500]\n",
    "ys = to_categorical(ys, 10)[:500]\n",
    "ys_ = to_categorical(ys_, 10)[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T17:48:35.067943Z",
     "start_time": "2021-05-20T17:48:35.050900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "iterations = 50\n",
    "batch_size = 500  # modified to evenly divide dataset size\n",
    "\n",
    "init_lr = 3e-5\n",
    "lr_div = 10\n",
    "lr_div_steps = set([300000, 400000])\n",
    "\n",
    "update_prob = 0.2\n",
    "validation_checkpoint = 10  # How often (iterations) to validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T17:48:36.801657Z",
     "start_time": "2021-05-20T17:48:36.790197Z"
    }
   },
   "outputs": [],
   "source": [
    "# Functions for constructing layers\n",
    "def dense_layer(inputs, units, name, output=False):\n",
    "    with tf.compat.v1.variable_scope(name):\n",
    "        x = tf.layers.dense(inputs, units, name=\"fc\")\n",
    "        if not output:\n",
    "            x = tf.layers.batch_normalization(x, name=\"bn\")\n",
    "            x = tf.nn.relu(x, name=\"relu\")\n",
    "    return x\n",
    "\n",
    "\n",
    "def sg_module(inputs, units, name, label):\n",
    "    with tf.compat.v1.variable_scope(name):\n",
    "        inputs_c = tf.concat([inputs, label], 1)\n",
    "        x = tf.layers.dense(inputs_c,\n",
    "                            units,\n",
    "                            name=\"fc\",\n",
    "                            kernel_initializer=tf.zeros_initializer())\n",
    "    return x\n",
    "\n",
    "\n",
    "def train_layer_n(n, h_m, h_n, d_hat_m, class_loss, d_n=None):\n",
    "    with tf.variable_scope(\"layer\" + str(n)):\n",
    "        layer_grads = tf.gradients(h_n, [h_m] + layer_vars[n - 1], d_n)\n",
    "        layer_gv = list(zip(layer_grads[1:], layer_vars[n - 1]))\n",
    "        layer_opt = tf.train.AdamOptimizer(\n",
    "            learning_rate=learning_rate).apply_gradients(layer_gv)\n",
    "    with tf.variable_scope(\"sg\" + str(n)):\n",
    "        d_m = layer_grads[0]\n",
    "        sg_loss = tf.divide(tf.losses.mean_squared_error(d_hat_m, d_m),\n",
    "                            class_loss)\n",
    "        sg_opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(\n",
    "            sg_loss, var_list=sg_vars[n - 1])\n",
    "    return layer_opt, sg_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T17:48:38.828328Z",
     "start_time": "2021-05-20T17:48:38.683226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-fba04ad23a13>:4: dense (from tensorflow.python.keras.legacy_tf_layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\hicss\\lib\\site-packages\\tensorflow\\python\\keras\\legacy_tf_layers\\core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-fba04ad23a13>:6: batch_normalization (from tensorflow.python.keras.legacy_tf_layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"architecture\"):\n",
    "    # Inputs\n",
    "    with tf.variable_scope(\"input\"):\n",
    "        X = tf.placeholder(tf.float32, shape=(None, 784), name=\"data\")  # Input\n",
    "        Y = tf.placeholder(tf.float32, shape=(None, 10),\n",
    "                           name=\"labels\")  # Target\n",
    "\n",
    "    # Inference layers\n",
    "    h1 = dense_layer(X, 256, \"layer1\")\n",
    "    h2 = dense_layer(h1, 256, name=\"layer2\")\n",
    "    h3 = dense_layer(h2, 256, name=\"layer3\")\n",
    "    logits = dense_layer(h3, 10, name=\"layer4\", output=True)\n",
    "\n",
    "    # Synthetic Gradient layers\n",
    "    d1_hat = sg_module(h1, 256, \"sg2\", Y)\n",
    "    d2_hat = sg_module(h2, 256, \"sg3\", Y)\n",
    "    d3_hat = sg_module(h3, 256, \"sg4\", Y)\n",
    "\n",
    "# Collections of trainable variables in each block\n",
    "layer_vars = [\n",
    "    tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                      scope=\"architecture/layer1/\"),\n",
    "    tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                      scope=\"architecture/layer2/\"),\n",
    "    tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                      scope=\"architecture/layer3/\"),\n",
    "    tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                      scope=\"architecture/layer4/\")\n",
    "]\n",
    "sg_vars = [\n",
    "    None,\n",
    "    tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                      scope=\"architecture/sg2/\"),\n",
    "    tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                      scope=\"architecture/sg3/\"),\n",
    "    tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                      scope=\"architecture/sg4/\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T17:48:42.504033Z",
     "start_time": "2021-05-20T17:48:41.889321Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ops for training\n",
    "with tf.variable_scope(\"train\"):\n",
    "    with tf.variable_scope(\"learning_rate\"):\n",
    "        learning_rate = tf.Variable(init_lr, dtype=tf.float32, name=\"lr\")\n",
    "        reduce_lr = tf.assign(learning_rate,\n",
    "                              learning_rate / lr_div,\n",
    "                              name=\"lr_decrease\")\n",
    "\n",
    "    pred_loss = tf.losses.softmax_cross_entropy(onehot_labels=Y,\n",
    "                                                logits=logits,\n",
    "                                                scope=\"prediction_loss\")\n",
    "\n",
    "    # Optimizers when using synthetic gradients\n",
    "    with tf.variable_scope(\"synthetic\"):\n",
    "        layer4_opt, sg4_opt = train_layer_n(4, h3, pred_loss, d3_hat,\n",
    "                                            pred_loss)\n",
    "        layer3_opt, sg3_opt = train_layer_n(3, h2, h3, d2_hat, pred_loss,\n",
    "                                            d3_hat)\n",
    "        layer2_opt, sg2_opt = train_layer_n(2, h1, h2, d1_hat, pred_loss,\n",
    "                                            d2_hat)\n",
    "        with tf.variable_scope(\"layer1\"):\n",
    "            layer1_opt = tf.train.AdamOptimizer(\n",
    "                learning_rate=learning_rate).minimize(h1,\n",
    "                                                      var_list=layer_vars[0],\n",
    "                                                      grad_loss=d1_hat)\n",
    "\n",
    "    # Optimizer when using backprop\n",
    "    with tf.variable_scope(\"backprop\"):\n",
    "        backprop_opt = tf.train.AdamOptimizer(\n",
    "            learning_rate=learning_rate).minimize(pred_loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T17:48:42.706369Z",
     "start_time": "2021-05-20T17:48:42.691369Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ops for validation and testing (computing classification accuracy)\n",
    "with tf.variable_scope(\"test\"):\n",
    "    preds = tf.nn.softmax(logits, name=\"predictions\")\n",
    "    correct_preds = tf.equal(tf.argmax(preds, 1),\n",
    "                             tf.argmax(Y, 1),\n",
    "                             name=\"correct_predictions\")\n",
    "    accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32),\n",
    "                             name=\"correct_prediction_count\") / 500\n",
    "\n",
    "# Ops for tensorboard summary data\n",
    "with tf.variable_scope(\"summary\"):\n",
    "    cost_summary_opt = tf.summary.scalar(\"loss\", pred_loss)\n",
    "    accuracy_summary_opt = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T17:48:45.538142Z",
     "start_time": "2021-05-20T17:48:44.795370Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 80.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train using backprop as benchmark\n",
    "with backprop_sess.as_default():\n",
    "    backprop_train_writer = tf.summary.FileWriter(\"logging1/backprop/train\")\n",
    "    backprop_validation_writer = tf.summary.FileWriter(\n",
    "        \"logging1/backprop/validation\")\n",
    "\n",
    "    backprop_sess.run(init)\n",
    "    for i in tqdm(range(1, iterations + 1)):\n",
    "        if i in lr_div_steps:  # Decrease learning rate\n",
    "            backprop_sess.run(reduce_lr)\n",
    "\n",
    "        data, target = xs, ys\n",
    "        _, summary = backprop_sess.run([backprop_opt, summary_op],\n",
    "                                       feed_dict={\n",
    "                                           X: data,\n",
    "                                           Y: target\n",
    "                                       })\n",
    "        backprop_train_writer.add_summary(summary, i)\n",
    "\n",
    "        if i % validation_checkpoint == 0:\n",
    "            Xb, Yb = xs_, ys_\n",
    "            summary = backprop_sess.run([summary_op], feed_dict={\n",
    "                X: Xb,\n",
    "                Y: Yb\n",
    "            })[0]\n",
    "            backprop_validation_writer.add_summary(summary, i)\n",
    "\n",
    "    # Cleanup summary writers\n",
    "    backprop_train_writer.close()\n",
    "    backprop_validation_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T17:48:58.812392Z",
     "start_time": "2021-05-20T17:48:56.773390Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 53.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train using synthetic gradients\n",
    "with backprop_sess.as_default():\n",
    "    sg_train_writer = tf.summary.FileWriter(\"logging1/sg/train\",\n",
    "                                            backprop_sess.graph)\n",
    "    sg_validation_writer = tf.summary.FileWriter(\"logging1/sg/validation\")\n",
    "\n",
    "#     sg_sess.run(init)\n",
    "    for i in tqdm(range(1, iterations + 1)):\n",
    "        if i in lr_div_steps:  # Decrease learning rate\n",
    "            sg_sess_1.run(reduce_lr)\n",
    "\n",
    "        data, target = xs, ys\n",
    "        with sg_sess_1.as_default():\n",
    "            sg_sess_1.run(init)\n",
    "            with tf.device(\"/job:local/task:0\"):\n",
    "                # Each layer can now be independently updated (could be parallelized)\n",
    "                if random.random(\n",
    "                ) <= update_prob:  # Stochastic updates are possible\n",
    "                    sg_sess_1.run([layer1_opt], feed_dict={X: data, Y: target})\n",
    "                if random.random() <= update_prob:\n",
    "                    sg_sess_1.run([layer2_opt, sg2_opt],\n",
    "                                feed_dict={\n",
    "                                    X: data,\n",
    "                                    Y: target\n",
    "                                })\n",
    "\n",
    "        with sg_sess_2.as_default():\n",
    "            sg_sess_2.run(init)\n",
    "            with tf.device(\"/job:local/task:1\"):\n",
    "                if random.random() <= update_prob:\n",
    "                    sg_sess_2.run([layer3_opt, sg3_opt],\n",
    "                                feed_dict={\n",
    "                                    X: data,\n",
    "                                    Y: target\n",
    "                                })\n",
    "                if random.random() <= update_prob:\n",
    "                    _, _, summary = sg_sess_2.run(\n",
    "                        [layer4_opt, sg4_opt, summary_op],\n",
    "                        feed_dict={\n",
    "                            X: data,\n",
    "                            Y: target\n",
    "                        })\n",
    "                    sg_train_writer.add_summary(summary, i)\n",
    "\n",
    "        if i % validation_checkpoint == 0:\n",
    "            Xb, Yb = xs_, ys_\n",
    "            summary = sg_sess_2.run([summary_op], feed_dict={X: Xb, Y: Yb})[0]\n",
    "            sg_validation_writer.add_summary(summary, i)\n",
    "\n",
    "    # Cleanup summary writers\n",
    "    sg_train_writer.close()\n",
    "    sg_validation_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T22:32:32.228704Z",
     "start_time": "2021-05-06T22:32:32.208673Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "sg_sess.close()\n",
    "backprop_sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T23:11:30.468225Z",
     "start_time": "2021-05-06T23:10:30.109932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ERROR: Timed out waiting for TensorBoard to start. It may still be running as pid 8588."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logging/sg/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
